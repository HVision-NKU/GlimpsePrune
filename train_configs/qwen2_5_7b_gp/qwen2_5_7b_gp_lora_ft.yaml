model_name_or_path: Qwen/Qwen2.5-VL-7B-Instruct
train_dataset: dataset_configs/viscot_rand.yaml
kd_weight: 0.04
reward_weight: 1.0
loc_weight: 0.0
le_weight: 0.0
use_attention_logits: true
le_layers:
  - 0
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
  - 8
  - 9
  - 10
  - 11
  - 12
  - 13
  - 14
  - 15
  - 16
  - 17
  - 18
  - 19
  - 20
  - 21
  - 22
  - 23
  - 24
  - 25
  - 26
  - 27
selected_layers:
  - 18
reduce_layer: 18



attn_fuse_type: AttnFuserV1
attn_fuse_size: 256
visual_cond_size: 512
attn_fuse_num_heads: 4
attn_fuse_hidden_act: silu
ori_attn_supervision: false
deep_supervision: false
attn_fuse_global: true
selected_visual_layers:
  - 31
  - 23
  - 15
  - 7


output_dir: output/qwen2_5_7b_gp_lora_ft_0801

# RL config
max_completion_length: 128
max_seq_length: 640
max_input_remain_seq_length: 512
max_input_seq_length: 6000
num_generations: 4

# Lora config
use_peft: true
lora_r: 32
lora_alpha: 64
lora_dropout: 0.05
lora_task_type: CAUSAL_LM


bf16: true
torch_dtype: bfloat16
attn_implementation: flash_attention_2
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1e-4
num_train_epochs: 1
lr_scheduler_type: cosine
warmup_ratio: 0.1

remove_unused_columns: false
logging_steps: 10
save_steps: 500